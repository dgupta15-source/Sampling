# Sampling
# Sampling Assignment – Probabilistic Sampling Techniques

# Objective
The objective of this assignment is to study the importance of sampling techniques in machine learning, especially when working with highly imbalanced datasets. The assignment analyzes how different probabilistic sampling techniques affect the accuracy of various machine learning models.

---

# Dataset
The dataset used is the **Credit Card Fraud Detection dataset**, which is highly imbalanced in nature.

- Target variable: `Class`
  - `0` → Non-fraudulent transaction
  - `1` → Fraudulent transaction

Due to the imbalance, applying sampling techniques is necessary to obtain reliable model performance.

---

# Sampling Techniques Used
The following **probabilistic sampling techniques** were implemented:

1. **Simple Random Sampling**  
   Randomly selects a subset of data from the population.

2. **Systematic Sampling**  
   Selects every *k-th* record from the dataset.

3. **Stratified Sampling**  
   Ensures that both classes are represented proportionally in train and test sets.

4. **Cluster Sampling**  
   Data is clustered using K-Means, and one cluster is selected as the sample.

5. **Cross Validation**  
   Stratified K-Fold cross-validation is used to evaluate model performance.

6. **Bootstrap Sampling**  
   Sampling with replacement to create a dataset of the same size as the original.

---

# Machine Learning Models Used
Five different machine learning models were applied to each sampling technique:

- M1: Logistic Regression  
- M2: Decision Tree Classifier  
- M3: Random Forest Classifier  
- M4: K-Nearest Neighbors (KNN)  
- M5: Naive Bayes Classifier  

---

# Methodology
1. Load and explore the dataset.
2. Apply each sampling technique separately.
3. Split sampled data into training and testing sets.
4. Train all five models on each sampled dataset.
5. Evaluate performance using **Accuracy Score**.
6. Handle cases where a sampled dataset contains only one class by skipping model training for that combination.
7. Compare results across all sampling techniques and models.

---

# Results
The accuracy scores of each model under different sampling techniques are summarized in a comparison table generated by the code.
Sampling         Bootstrap  Cluster  Cross Validation  Simple Random  \
Model                                                                  
M1_Logistic       0.987097   1.0000          0.985748            1.0   
M2_DecisionTree   0.987097   0.9875          0.790943            1.0   
M3_RandomForest   1.000000   1.0000          0.989644            1.0   
M4_KNN            0.974194   1.0000          0.988345            1.0   
M5_NaiveBayes     0.974194   0.9750          0.946862            1.0   

Sampling         Stratified  
Model                        
M1_Logistic        0.987097  
M2_DecisionTree    0.967742  
M3_RandomForest    0.987097  
M4_KNN             0.987097  
M5_NaiveBayes      0.980645  


Key observations:
- Stratified Sampling and Cross Validation provide more stable results.
- Systematic and Cluster Sampling may sometimes exclude the minority class due to extreme imbalance.
- Bootstrap Sampling helps increase model robustness.
- Tree-based models generally perform better on imbalanced datasets compared to linear models.

---

# conclusion
This experiment demonstrates that sampling techniques play a critical role in handling imbalanced datasets. No single sampling method is universally best; the effectiveness depends on the chosen machine learning model. Proper sampling improves model reliability and prevents biased predictions.

---

## Repository Structure
